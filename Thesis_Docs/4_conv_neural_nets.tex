%\include{packages}
%\begin{document}

\chapter{Convolutional neural networks}\label{convnets}

Convolutional neural networks (\textsc{cnn}s) are of particular interest when working with image problems as the input data is assumed to be two-dimensional. In regular neural networks (see Chapter \ref{neuralNets-intro}), the neurons in each layer are connected to all the neurons of the previous layer. These layers are known as \textit{fully connected layers}. In \textsc{cnn}s, convolutional layers examine only small subimages of the entire image sample. Each pixel of an image forms a neuron, which is connected only to nearby pixels, rather than all pixels in the image. The \textsc{cnn} is trained to identify a set of visual features which can be combined and interpreted for image classification tasks.

% Diagram of typical CNN structure
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/4_cnn_structure.png}
	\caption{Typical CNN Structure. Convolution layers with ReLU activation (nonlinearity) are followed by pooling layers. The final layers are fully connected, using softmax activation to generate a probability distribution.}
	\small Image taken from \cite{ADeshpande2016}
	\label{convnets-structurefig}
\end{figure}

CNNs typically consist of alternating convolutional layers (see Section \ref{convnets-convlayer}) and max pooling layers (see Section \ref{convnets-pool}), as shown in Figure \ref{convnets-structurefig}. The convolutional layers identify significant visual features, however they introduce a large number of variables into the network. Max pooling layers occur after convolutional layers to reduce network dimensionality. For classification tasks, the last layers of the network are fully connected to ensure the final outputs match the structure of the given responses. Softmax activation (see Section \ref{nnets-act}) is used for the final output layer to generate a probability distribution of the classifications.

\section{Convolutional layers}\label{convnets-convlayer}

The neurons of convolutional layers have a similar overall structure to that of regular neurons (see Section \ref{nnets-structure}). The input variables $X$ are multiplied with weights $W$, summed and added to a bias variable $b$. The resulting linear combination is passed through an activation function $\sigma(\cdot)$ to give the output of the neuron with activation value $a$.

The major difference between convolutional layers and regular neural network layers is the structure of the inputs and weights. Image data is two-dimensional and generally has multiple colour \textit{channels}. For example, a coloured image contains three channels: red, green, and blue. The intensity of each pixel in each channel is represented as a number such that the image forms a three-dimensional array. The weights of convolutional layers are formatted as three-dimensional arrays to accommodate for the number of colour channels. These weight arrays are element-wise multiplied with a subset of the whole image. Unlike fully connected layers, convolutional layers do not apply weights to all of the input variables at once. Instead, the weights have an array height and width much smaller than that of the input image, designed to identify particular visual features in smaller subimages. A single matrix of weights that describes a feature is called a \textit{filter}. Each convolutional layer can have multiple filters, analogous to having multiple neurons in a fully connected layer.

Each filter processes the whole image by multiplying with subimages chosen sequentially. The process, known as \textit{convolution}, begins at the top-left of the image and moves towards the bottom-right. Let the $f$-th filter of the convolultional layer be $F^{(f)}$, with dimension $M\times N \times C$. Given an input image matrix $X$, number of channels $C$, and the bias of the $f$-th filter $b^{(f)}$, the value of the convolution at element $X_{j,k}$ is given by
\begin{align}
	a_{jk}^{(f)} = \sigma\left(\sum_{c=1}^C\sum_{m=0}^{M-1}\sum_{n=0}^{N-1}X_{j+m, k+n, c}F_{m,n,c}^{(f)}  + b^{(f)}\right).
\end{align}

Let us consider an example.
\begin{example}
%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.5]{Images/4_conv_eg2.png}
%\label{convnets-conv-eg}
%\end{figure}

\begin{figure}[h]
\centering
\begin{tikzpicture}[font=\sffamily]
\draw[step=1cm] (0, 0) grid (3, 3);
\draw[step=1cm] (5, 1) grid (7, 3);
\draw[step=1cm] (9, 2) grid (10, 3);

\node at (0.5, 2.5) {\color{blue}1};
\node at (1.5, 2.5) {\color{blue}0};
\node at (2.5, 2.5) {1};
\node at (0.5, 1.5) {\color{blue}2};
\node at (1.5, 1.5) {\color{blue}0};
\node at (2.5, 1.5) {2};
\node at (0.5, 0.5) {1};
\node at (1.5, 0.5) {1};
\node at (2.5, 0.5) {-1};

\node at (5.5, 2.5) {\color{red}1};
\node at (6.5, 2.5) {\color{red}0};
\node at (5.5, 1.5) {\color{red}-1};
\node at (6.5, 1.5) {\color{red}1};

\node at (9.5, 2.5) {\textcolor[rgb]{0.56,0.07,1}{1}};

\node at (1.5, 3.5) {Input $3\times3\times1$};
\node at (6, 3.5) {Filter $2\times2\times1$};
\node at (9.5, 3.5) {Bias};

\end{tikzpicture}
\end{figure}

The filter in red is multiplied element-wise with the inputs given in blue. This gives:
\begin{figure}[h]
\centering
\begin{tikzpicture}
\draw[step=1cm] (0,0) grid (2, 2);
\node at (0.5,1.5) {1};
\node at (1.5,1.5) {0};
\node at (0.5,0.5) {-2};
\node at (1.5,0.5) {0};
\end{tikzpicture}
\end{figure}

Taking the sum of the products and adding the bias in purple gives the activation value $(1 + 0 + -2 + 0) + 1 = 0$. This process is iterated for all $2\times2$ subimages in the original input, giving the following output:

\begin{figure}[h]
\centering
%\includegraphics[scale=0.5]{Images/4_conv_eg2_2.png}
\begin{tikzpicture}

\draw[step=1cm] (0,0) grid (2, 2);
\node at (0.5,1.5) {0};
\node at (1.5,1.5) {3};
\node at (0.5,0.5) {3};
\node at (1.5,0.5) {-1};

\node at (1, 2.5) {Output $2\times2\times1$};

\end{tikzpicture}
\end{figure}

\end{example}

% Diagram of convolution algorithm
%\begin{figure}[ht]
%	\centering
%	\includegraphics[scale=0.5]{Images/4_convolution.png}
%	\caption{Convolution algorithm with 2 filters of size 3x3, stride 2, zero padding 1}
%	\small Image adapted from \url{`http://cs231n.github.io/convolutional-networks/'}
%	\label{convnets-conv-alg}
%\end{figure}

%As shown in Figure \ref{convnets-conv-alg}, the convolution process starts from the top-left corner of the input volume. A subset of the image is taken at that location, with the same dimensions as the filter. The image subset and the filter are entry-wise multiplied and added together. A bias variable is added and an activation function applied as with fully connected layers. This result becomes the output of a single neuron of the convolutional layer. The filter is moved to the next location (depending on the size of the stride) to build the value of the next neuron.

The arrays output by the convolutional filters are bound together to form a three-dimensional array, with depth dependent on the number of filters in the layer.

The convolution process involves a very large number of variables, many of which are redundant when identifying important features. A number of techniques have been developed to control how the subimages are chosen, reduce image dimensionality, speed up processing, and provide flexiblity in network design. We now discuss some of these methods.

\subsection*{Zero padding}\label{convnets-pad}

During convolution, the filters are only placed within the boundaries of the input image, resulting in loss of dimension from the borders. For an input image with dimension $J \times K$ and a filter with dimension $M \times N$, the filter output will be of dimension $(J - M + 1)\times (K - N + 1)$. This places a limit on the number of convolutional layers used before the image dimension becomes too small for further convolution.

\textit{Zero padding} surrounds the edges of the input image with $P$ layers of zeros, increasing the input dimension to  $(J+2P) \times (K+2P)$. When convolution takes place, the resulting image size has dimension $(J+2P - M + 1) \times (K + 2P - N + 1)$. The value of $P$ can be set such that the convolution maintains the original image dimension, allowing the network to continue applying convolution to outputs, thus producing very deep networks \cite{GoogLeNet2015}.

\subsection*{Stride}\label{convnets-stride}

Convolving filters at all possible locations of large input images results in lengthy training time and large outputs. Applying the convolution of filters at every $S$-th subimage reduces the number of variables whilst maintaining use of the whole input image. The magnitude of the filter movement is known as the \textit{stride}.

An input image with dimension $J \times K$ convolved with a filter with dimension $M \times N$, with padding $P$ and stride $S$, will have an output dimension of $\left(\left\lfloor (J + 2P - M)/S\right\rfloor + 1\right) \times \left(\left\lfloor (K + 2P - N)/S \right\rfloor + 1\right)$.

\section{Pooling layers}\label{convnets-pool}

Convolutional layers are followed by pooling layers to reduce dimensionality whilst retaining variables that identify significant features \cite{ADeshpande2016}. The model is only interested in identifying variables for classification and does not need the location to be highly specific. A sliding window of size $M\times M$ and stride $S$ is applied to the top-left of the image input and moves across the image similarly to the convolutional layers. At each location, the pooling layer outputs a single value, which collectively form a matrix of reduced dimension to the input.

For \textit{max pooling}, the values output are the maximum values of all inputs in each sliding window. Max pooling is a preferred pooling method as it emphasises features of interest located by the previous convolutional layer.

\section{ReLU activation}\label{convnets-act}

The ReLU activation function, as defined in Section \ref{nnets-act}, is frequently used for  convolutional neural networks. Computation time can become very large as there are a large number of variables. The ReLU function can be computed very quickly and so is a common choice for convolutional neural networks \cite{ADeshpande2016}. Additionally, the ReLU activation function is also able to avoid the vanishing gradient problem (see Section \ref{nnet-vanishinggradprob}).


%Convolutional Neural Networks are very similar to traditional neural networks, but make the assumption that the input data are images.
%The neurons of a convNet are arranged in 3D. 
%http://cs231n.github.io/convolutional-networks/
%
%Input layer: raw pixel values, with width, height and color channels
%Conv layer: Compute output for regions of input. Each computes a dot product between weights and a region that they are connected to.
%ReLU: elementwise activation function.
%Pool: downsampling
%FC - fully connected: each neuron here is connected to all previous.
%
%Conv layer has a set of learnable filters. The filter might have only a small size, but will slide (convolve) across the image, computing dot products at each position.

%https://stats.stackexchange.com/questions/154879/a-list-of-cost-functions-used-in-neural-networks-alongside-applications
%https://pappubahry.com/misc/neural/nielsen_1/
% http://neuralnetworksanddeeplearning.com/

%Good animated representation: https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/
%
%https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8
%
%https://tensorflow.rstudio.com/tensorflow/articles/tutorial_mnist_beginners.html
%
%https://adeshpande3.github.io/A-Beginner%27s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/
%
%https://tech.hbc.com/2016-05-18-fully-connected-to-convolutional-conversion.html


%\section{R-CNN}
%Purpose is to take in an image, and draw bounding boxes over all of the objects. Train to find 4D output (x, y, width, height) of object. Use L2 distance loss between prediction and 'ground truth'.
%
%Done by attaching a fully connected layer to the last conv layer. Separate classification layers and box coord layers. 
%Accuracy determined by Intersection over Union (ioU) area. 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{bibliography}