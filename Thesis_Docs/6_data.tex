%\include{packages}
%\begin{document}

\chapter{Data sample collection}\label{data}

Lacunes are identified from \textsc{mri} scans of two weightings: T1-weighted and \textsc{flair} images. The known locations of lacunes are described in an accompanying spreadsheet. In this chapter, we describe the source and format of the scans (Section \ref{data-mri}), brain tissue extraction (Section \ref{data-soft}), the generation of response values from the given spreadsheet (Section \ref{data-lacune}), and the final structure of each sample (Section \ref{data-samples}).

\section{\textsc{mri} and preprocessing}\label{data-mri}

The \textsc{mri} and lacune location data sets were collected as a part of the Sydney Memory and Aging Study (Sydney \textsc{mas}) conducted at the University of New South Wales' Centre for Healthy Brain Ageing, and sourced from the second wave of \textsc{mas} scans. They were acquired using a Philips 3T Achieva Quasar Dual scanner (Philips Medical Systems, The Netherlands). For radiologists' reference, the scanning parameters for the T1-weighted and \textsc{flair} images are:

T1-weighted \textsc{mri} - TR = 6.39 ms, TE = 2.9 ms, flip angle = 8$^\circ$, matrix size = 256$\times$256, field of view = 256$\times$256$\times$190, and slice thickness = 1 mm with no gap in between, yielding 1$\times$1$\times$1 mm$^3$ isotropic voxels.

\textsc{flair} - TR = 10 000 ms, TE = 110 ms, TI = 2800 ms, matrix size = 512$\times$512, slice thickness = 3.5 mm without gap, and in-plane resolution = 0.488$\times$0.488 mm.

The \textsc{flair} images were transformed using \textsc{spm12} software, such that their coordinates correspond to those from the T1 scans.
%This was done using \textsc{spm12} software (\url{https://www.fil.ion.ucl.ac.uk/spm/software/spm12/}).

\section{Extracting soft tissue}\label{data-soft}

T1-weighted images have a high enough resolution that it is possible to identify patients through their face structure and eyes. Brain matter (soft tissue) masks were generated to remove features that are not part of the brain tissue and de-identify the data.

Individual T1 images were segmented into grey matter, white matter, and \textsc{csf} probability maps using the segmentation tool in \textsc{spm12}. Grey matter and white matter probabilities were summed and voxels at a threshold of 0.5 or greater were included in the soft tissue mask. These masks were applied to each of the T1-weighted scans, as shown in Figure \ref{data-t1-soft-fig}.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/6_t1_soft_eg.png}
\caption{Original T1-weighted image and the extracted soft tissue.}
\label{data-t1-soft-fig}
\end{figure}

The resulting images retain voxels that are likely to contain brain tissue. Other features, such as the skull and suspected \textsc{csf}, are given an intensity of zero. Lacunes are filled with fluid and so have a signal intensity similar to that of \textsc{csf}. Consequently, lacunes are removed by the soft tissue mask. The removed regions tend to have well-defined edges, which may aid in the identification of possible lacunes. Note that lacunes are still visible in the \textsc{flair} images.

\section{Generating response arrays}\label{data-lacune}

The T1-weighted and \textsc{flair} scans were rated visually by trained clinicians in accordance to the \textsc{strive} criterion \cite{WardlawJ.M.2013Nsfr}. The clinicians visually analysed the scans slice by slice, identifying possible lacunes, perivascular spaces, and other lesions. Each lesion was analysed by a team of clinicians to confirm the identification. The rating of lacunes were then logged in Microsoft Excel, which also details the scan ID and the number of lacunes in each \textsc{mri} scan. For each lacune detected, the spreadsheet lists the axial slice ($y$-coordinate), diameter in millimetres, hemispheric location, and the ID of the surrounding brain structure. A sample of the spreadsheet data is shown in Table \ref{data-excel-tab}.

\begin{table}[ht]
	\centering
	\begin{tabular}{llllll}
	\toprule[1.5pt]
	Scan ID & No. of lacunes & Axial Slice & Diameter & Side & Region\\
	\midrule
	42 & 2 & 102 & 7 & L & 3 (Cortex)\\
	102 & 1 & 112 & 10 & R & 4 (Thalamus)\\
	\ldots\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Sample spreadsheet data. Each row describes one scan, identifying the size and location of lacunes. Each row contains repeated columns to describe multiple lacunes per row.}
	\label{data-excel-tab}
\end{table}

The provided data describes the approximate anatomical location of lacunes. This format is not immediately usable to the model as it lacks precise coordinates. To resolve this, the spreadsheet was used as a guide to visually identify lacunes. Once found, an overlay of lacunes for the T1-weighted scans was generated such that each pixel in the image corresponds to a positive binary response.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/6_lacune_mask.png}
\caption{Comparison of T1-weighted images, corresponding \textsc{flair} and lacune identification overlay (lacune in red).}
\label{data-t1-flair-lac}
\end{figure}

The responses were generated in FSLView, a program used by neuroscientists to view and annotate \textsc{mri} scans in the \texttt{.nifti} file format. For each brain scan described in the Excel spreadsheet, FSLView was used to generate a zero-initialised three-dimensional array of the same dimensions as the corresponding T1-weighted scan. Lacunes were visually identified by examining the indicated brain structure for lesions that appear dark in the T1-weighted images and with a hyperintense rim in the \textsc{flair} images. The empty arrays were opened in FSLView such that the voxels correspond to that of the T1-weighted images. The voxels that form the identified lacunes were indicated with ones using the Brush tool. These overlays were saved as \texttt{.nifti} files so they could be imported alongside the soft-tissue and \textsc{flair} \texttt{.nifti} files. An example of a lacune overlay is shown in Figure \ref{data-t1-flair-lac}.



% Start descriptions of data - where it came from. Rating process and wave reviews. Data itself consisted of t1 and flair scans, and excel spreadhseet of slice numbers and sizes. To build the response values, an overlay was made for each scan. Overlays were built in fslview in the nifti format. Lacunes were identified using the guidance spreadsheet. A brush tool was used to fill 1s for lacunes. 0 elsewhere.

\section{Generating samples}\label{data-samples}

The candidate generation model by \cite{GhafoorianM.2017Dml3} specifies each sample to be 51$\times$51 axial images of both T1-weighting and \textsc{flair}. In their model, samples were chosen randomly such that positive lacune samples encompassed one-third of the data set. Data augmentation was used to increase the number of samples. In total, Ghafoorian et al. collected $3.2\times10^5$ training samples from 1075 scans.

Our data set contains significantly fewer scans. In total the data set contains 411 \textsc{mri} scans, of which 35 contain lacunes. The scans were imported into R and converted into three-dimensional arrays using the AnalyzeFMRI package (v1.1-17). Each value of an array is the signal intensity of the scan at that voxel. Regions external to the scanned brain are given an intensity of zero.

The locations of the positive samples (lacunes) were extracted using the overlays generated in Section \ref{data-lacune}. For each nonzero value in the overlay, two 51$\times$51-dimensional arrays were created from the corresponding T1-weighted and \textsc{flair} images. The pixel to be classified occurs at the centre of each array. To increase sample size, additional augmented samples were formed by flipping the image horizontally. This method of sampling returned 3846 lacune samples in total. Examples are shown in Figure \ref{data-positives}.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/6_positives.png}
\caption{Examples of positive samples. T1-weighted images and corresponding \textsc{flair}.}
\label{data-positives}
\end{figure}


\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/6_negatives.png}
\caption{Examples of negative samples. T1-weighted images and corresponding \textsc{flair}.}
\label{data-negatives}
\end{figure}

Negative (non-lacune) samples were generated by considering voxels that return a negative response in the lacune overlay. The number of lacunes appearing in \textsc{mri} is fairly low, and each lacune has a diameter up to only 15 mm. Therefore, the number of potential negative samples vastly outnumbers positive samples. Given too many negative samples, the model will have a tendency to classify each given sample as negative without the cost function reporting a large errors. For example, a data set containing 99\% negatives will return a 99\% classification accuracy for a model that outputs all points as negative.

Negative samples were chosen in intervals of 25 pixels with starting location chosen randomly. Samples were discarded if the central voxel was surrounded by a 4$\times$4$\times$4 empty volume, reducing the number of sparse samples. Examples are shown in Figure \ref{data-negatives}. This was used to generate a total of 39983 negative samples. Positives samples make up 8.78\% of the dataset.

%\section{The Data}
%
%Where the data came from. The MRI source, type of images, with wavelengths etc. Similar description to Ghafoorian's (Section 2.1). Any preprocessing.
%
%What the samples were. E.g. 51x51 patches. Number of training, validation, testing.
%
%Show some example images, alongside their classification.
%
%The system that the models were built under - built in R using a tensorflow API. Models were run on dedicated servers.

%\section{First Model Structure}
%
%Code in Appendix.
%
%Purpose was to have a point of comparison against the model built by Ghafoorian et al. \cite{GhafoorianM.2017Dml3}.
%
%Brief outline of structure.
%
%Different number of samples. Far fewer lacunes in our dataset. Paper had 2/3 negatives, 1/3 positives. Our data consists of just under 10\% positives. Proposed model paper had 320K total samples. Our data has 50K, far fewer samples than the proposed model.
%
%In addition, around the 11th epoch, the training accuracy drops from near 100\% to near 0\%. This could be since the cross entropy has a log, and the algorithm attempts to take log(0). Introduce a small constant to achieve log(y + 1e-10).
%Getting NANs from the cross entropy function.
%
%
%\section{Proposed Model Structure}
%
%Code in Appendix.
%
%Explain structure of model, including diagram similar to that from Ghafoorian. Number of layers, number of neurons in each layer. What each layer was and the order. Method for chosen hyper-parameters. 


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{bibliography}