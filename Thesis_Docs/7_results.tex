%\include{packages}
%\begin{document}

\chapter{Results}\label{results}

\section{Final model structure}

The final simplified model has a similar structure to that of Ghafoorian et. al's candidate detection model \cite{GhafoorianM.2017Dml3}. The model structure is shown in Figure \ref{results-model-fig}. The input data contains two axial images: soft-tissue extracted T1 and FLAIR images. Each of the images has resolution 51$\times$51 pixels and is centered at the same coordinate. The model classifies the central pixel as either a positive or negative reading, formatted as a 2-dimensional vector. A response of $(1,0)^\intercal$ indicates a positive (lacune) sample and $(0,1)^\intercal$ a negative (non-lacune) sample.

% Simplified model structure
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/7_simplified_model.png}
	\caption{Simplified model structure.}
	\small Image adapted from \cite{GhafoorianM.2017Dml3}
	\label{results-model-fig}
\end{figure}

The model consists of four convolutional layers of size 20, 40, 80 and 110, with filter sizes 7$\times$7, 5$\times$5, 3$\times$3 and 3$\times$3 respectively. A single max pooling layer is placed after the first convolutional layer, with size 2$\times$2 and stride 2. The convolutional layers are followed by three fully connected layers of size 300, 200 and 2. The final layer has softmax activation to output a probability distribution. All other layers have ReLU activation in order to avoid the vanishing gradient problem (see Section \ref{nnet-vanishinggradprob}).

All neurons undergo batch normalisation and are initialised using the He method \cite{HeKaiming2015DDiR}. A dropout rate of 0.3 was applied to the fully connected layers.

Training was conducted using stoachastic gradient descent with Adam optimiser and a batch size of 128. A decaying learning rate was set from $5\times10^{-4}$ to $1\times10^{-6}$ by the 40th epoch. Cross-entropy loss was used with L2-regularisation. The penalty rate was set to $1\times10^{-4}$. 

An early stopping mechanism was implemented such that the model is tested against the validation set at the end of each epoch. If the validation accuracy improves, the model is saved. Once 40 epochs have run, the final saved model will have the highest validation accuracy.

Two models were trained using different positive-negative response ratios. The first model was developed using the positive-negative response ratio described by Ghafoorian et. al \cite{GhafoorianM.2017Dml3}: one third positive and two thirds negative. There are only 3846 positive samples in the MAS dataset, allowing for a random selection of 7692 negative samples. The total sample size is 11538. These samples are split into three datasets: training, validation and testing. Splitting the data into these groups with a ratio of 50:25:25 yields sample sizes 5769, 2884 and 2885 respectively.

The second model used the entire developed set of positive and negative lacune samples. This consisted of 3846 positive and 40008 negative samples to give a total sample size of 43854. Positives encompassed 8.77\% of data samples. Similarly to the first dataset, the data was split into training, validation and testing sets using a ratio of 50:25:25. This yielded sample sizes 21927, 10963 and 10964 respectively. All datasets were saved as R data files (\texttt{.Rda}).


\section{Training environment}

Model code was developed in R (v3.5.0) using RStudio (v1.1.453). The neural network was built and trained using Tensorflow (v1.10.0) through the R interface \texttt{tensorflow} (v1.8). The model was trained on a Linux machine running Ubuntu (release 16.04). 

\section{Results}

\subsection*{One Third Positives}

The first model generated had the same positive-negative sample ratio as outlined by Ghafoorian et. al \cite{GhafoorianM.2017Dml3}. The data consisted of one third positives and two thirds negatives. The resulting samples were split into three datasets: training, validation and testing in the ratio of 50:25:25.

% Training accuracy
\begin{figure}[b]
	\centering
	\includegraphics[width=\textwidth]{Images/7_train_acc4.pdf}
	\caption{Training data accuracy logged every 5 batches}
	\label{results-train-acc4-fig}
\end{figure}

Each epoch of training uses the entire training dataset. The model is trained for 40 epochs over 23 minutes, adjusting the network weights with each batch of 128 samples. Reusing the same data samples a large number of times introduces overfitting into the network \cite{Goodfellow-et-al-2016}. Overfitting can be observed when the accuracy of the training dataset is very high, whereas the accuracy of the validation and testing datasets are low. Hence assessing model training accuracy can efficiently convey early model improvement however is not indicative of the model's performance when given new data.

Training accuracy was calculated every 5 batches, where each batch contained 128 samples. The resulting accuracies are shown in Figure \ref{results-train-acc4-fig}. There is a rapid increase in training accuracy within the first 100 batches, reaching 100\% before batch 250. The model improvement occurs in steps as new features are found and more precise weight changes are made with the lowering learning rate \cite{Folly2009, Nielson2015}. The model reports a 100\% training accuracy consistently by batch 300. A majority of the model training occurs very early. In later training epochs, the training cost and learning rates are low and so the change in weights is also low (see Section \ref{nnets-backprop} on Backpropagation).

% Validation accuracy
\begin{figure}[b]
	\centering
	\includegraphics[width=\textwidth]{Images/7_valid_acc4.pdf}
	\caption{Validation data accuracy logged after each epoch}
	\label{results-valid-acc4-fig}
\end{figure}

The model's validation set accuracy was tested after each epoch. This dataset was not used for training and is used to assess the model's performance given new unseen data. The resulting validation accuracies are shown in Figure \ref{results-valid-acc4-fig}. Validation accuracy increases rapidly for the first 10 epochs before a plateau at 20 epochs. The maximum validation accuracy occurred at epochs 31 and 34, achieving an accuracy of 0.9899445. The model was saved at epoch 31 to help minimise overfitting that may occur at later epochs.

Applying this best validation accuracy model to the test dataset resulted in an accuracy of 0.9885615. Though overall accuracy is a strong indicator of model performance, some types of inaccuracy can be more detrimental than others. In the identification of lacunes, it is preferable to have a larger number of false candidate lacunes than misclassify a true lacune. As a result, models will be compared by considering actual and predicted classification differences. Using this scheme, there are four outcomes: true positives, true negatives, false positives, and false negatives. These outcomes can be formatted into a \textit{confusion matrix}, as shown in Table \ref{results-confmat4-tab}.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lll@{}}
	\toprule[1.5pt]
	& Positive & Negative\\
	\midrule
	True & 972 & 1879\\
	False & 30 & 4\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Confusion matrix of the testing dataset.}
	\label{results-confmat4-tab}
\end{table}

The sensitivity of the model to lacunes is 99.6\% and specificity for non-lacunes is 98.4\%.

\subsection*{8.77\% Positives}

The second model was built using all of the generated positive and negative samples as described in Section \ref{data-samples}.

The resulting training accuracy is shown in Figure \ref{results-train-acc5-fig}. The additional noise present in comparison to the previous model is the result of the larger sample size, rising from 5769 to 21927 samples. Note that the batch size and number of training epochs remained at 128 and 40 respectively, resulting in a larger number of batches processed during training. Model training duration was 1 hour and 13 minutes. The behaviour of this model was similar to that of the previous model, with a steep increase in training accuracy in early epochs followed by consistent correct classifications. Training accuracy first reached 100\% at batch 205 and was consistently 100\% by batch 750. 

% Training accuracy
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/7_train_acc5.pdf}
	\caption{Training data accuracy logged every 5 batches}
	\label{results-train-acc5-fig}
\end{figure}

The performance of the model on the validation set is given in Figure \ref{results-valid-acc5-fig}. Validation accuracy was maximised at epoch 22, achieving an accuracy of 0.9983581. 

% Validation accuracy
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/7_valid_acc5.pdf}
	\caption{Validation data accuracy logged after each epoch}
	\label{results-valid-acc5-fig}
\end{figure}

Applying this best validation accuracy model to the test set achieved an accuracy of 0.9968989. The confusion matrix is given by Table \ref{results-confmat5-tab}, to give a sensitivity of 99.3\% and specificity of 99.7\%.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lll@{}}
	\toprule[1.5pt]
	& Positive & Negative\\
	\midrule
	True & 955 & 9969\\
	False & 33 & 7\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Confusion matrix of the testing dataset.}
	\label{results-confmat5-tab}
\end{table}

\subsection*{Model comparisons}

Between the two trained models, the model that was trained on more data had slightly less sensitivity but much greater specificity. A hypothesis test comparing the sensitivity and specificity rates between the two models is conducted.

First test the difference in sensitivity rates. The Central Limit Theorem (\textsc{clt}) does not apply as the number of false negatives is less than 10 for both models. Consider the pooled sensitivity probability $p_{pooled} = \dfrac{972+955}{972+4+955+7} = 0.994324$. Test the hypothesis that both models exhibit sensitivities at the rate of $p_{pooled}$. Let $X_1$ and $X_2$ be the number of lacunes correctly identified under the first and second models respectively. The models classify each sample independently and the data are sampled randomly. Under these assumptions we can model $X_1$ and $X_2$ using the Binomial distribution. Under the null hypothesis, $X_1 \sim Bin(976, p_{pooled})$ and $X_2 \sim Bin(962, p_{pooled})$. The probability of the first model exceeding 972 correct classifications out of 976 is $P(X_1 \geq 972) = 0.19643$. The probability of the second model having fewer than 955 correct classifications out of 962 is $P(X_2 \leq 955) = 0.30749$. At the 5\% level, neither of these sensitivities are significantly different from $p_{pooled}$ and so are not significantly different from each other.

Now test the difference in specificity rates. Hypothesise that both sample proportions come from sampling distributions with the same probability. The number of true negatives and false positives are above 10 for both models. The data was sampled randomly and lacune classifications were made independently of other samples. Under these assumptions, the two-proportion z-test can be applied. Let $X_1$ and $X_2$ be the number of correctly classified negative samples from the first and second models respectively. Let $n_1$ and $n_2$ be the total number of negative samples so that $\hat{p}_1 = \dfrac{X_1}{n_1}$ and $\hat{p}_2 = \dfrac{X_2}{n_2}$ are the observed specificities. The observed pooled specificity proportion is $\hat p_{pooled} = \dfrac{9969+1879}{9969+33+1879+30} = 0.99471$. From the \textsc{clt},
\[
	Z = \dfrac{\hat{p}_1 - \hat{p}_2}{\sqrt{p_{pooled}(1 - p_{pooled})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}} \approx \mathcal{N}(0,1).
\]
The resulting test statistic is $z = -6.8533$. The probability of the observed proportion difference or lower is given by $P(Z < z) = 3.61\times10^{-12}$. This is well below the 5\% significance level and it is concluded that the specificity of the second model is higher than that of the first.

It has been proven that the model trained with the larger sample size and smaller proportion of positives did not affect sensitivity rates and increased specificity. 

\todo[inline]{Show examples of false positives and true negatives. What do these tend to look like?}

\todo[inline]{What happens when the scan is run on a whole brain?}

%\section{Reference Model Results}
%
%Results from running Ghafoorian's model. Training time, and final sensitivity and average false-positives per slice.
%
%\subsection*{Attempt1}
%
%training/testing in ratio 70:30. Here, only 7\% of data consists of positives. 41819 samples in total. Validation occurs on first 500 of the testing set. Training time: 02:04:48.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 8, with an accuracy of 100\%. Though should be noted that the size of this validation set consists of only 500 samples. Testing the whole testing set (15544 samples) achieves an accuracy of 0.9938.
%
%\subsection*{Attempt2}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11539 samples in total. Training time: 00:23:11.
%Training accuracy achieved 100\% before batch 50x5. Validation accuracy peaked at epoch 21, which achieved an accuracy of 0.984055. Applying this best validation model to testing data achieved an accuracy of 0.9833622. Lower accuracy could be the result of fewer data points for training - caused by having only a limited number of positives in the original dataset compared to the abundant negatives.
%
%\subsection*{Attempt3}
%
%Use original 7\% ratio. 41819 samples, with training/validation/testing in ratio 50:25:25. Have to be careful not to introduce too many negatives as this could impact what the model attempts to predict. E.g. if there are only 1\% positives, then an accuracy of 99\% could be achieved just by labelling all the samples as negative. Training time: 01:58:23.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 18, with an accuracy of 0.9979159, with the set containing 12955 samples. The testing set achieved an accuracy of 0.9972.
%
%\subsection*{Fixed Samples}
%
%Samples were changed. Previously, negative samples were chosen such that they were not lacunes, and also did not have a central pixel value of 0. However, some regions within the brain matter do have a central value of 0, and these can be some of the hardest points to distinguish.
%
%Negative sampling was changed to random points that were not lacunes, and the entire 9x9 central square is not 0 (to remove samples from outside the brain matter). 
%
%\subsection*{Attempt4}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11538 samples in total. Training time: 00:22:51.
%Training accuracy achieved 100\% at batch 28x5. Consistent 100\% by batch 50x5. Validation accuracy peaked at epochs 31 and 34, achieving an accuracy of 0.9899445. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9885615.
%
%Testing set was only 2885 samples. The number of true/false positives and negatives were:
%TP: 972
%TN: 1879
%FP: 30
%FN: 4
%
%Sensitivity is 972/(972+4) = 99.6\% and specificity is 1879/(1879+30) = 98.4\%.
%
%
%
%\subsection*{Attempt5}
%
%training/validation/testing with 8.77\% positives. 43854 samples in total. Training time: 01:13:17. Training accuracy achieved 100\% at batch 41x5=205. Consistently 100\% at batch 150x5=750. Validation accuracy peaked at epoch 22, which achieved an accuracy of 0.9983581. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9968989.
%Testing set comprised of 10964 samples. Of these, the number of true/false positives and negatives were calculated:
%
%TP: 955
%TN: 9969
%FP: 33
%FN: 7
%
%Sensitivity is 955/(955+7) = 99.3\%, and specificity is 9969/(9969+33) = 99.7\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{bibliography}