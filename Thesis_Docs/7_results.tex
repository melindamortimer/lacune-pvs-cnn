%\include{packages}
%\begin{document}

\chapter{Results}\label{results}

\section{Final model structure}



\section{Training environment}

% R: RStudio - tensorflow, Linux, 

\section{Results}



\citep{AdamsH.H.Hieab2013RMfD}

%\section{Reference Model Results}
%
%Results from running Ghafoorian's model. Training time, and final sensitivity and average false-positives per slice.
%
%\subsection*{Attempt1}
%
%training/testing in ratio 70:30. Here, only 7\% of data consists of positives. 41819 samples in total. Validation occurs on first 500 of the testing set. Training time: 02:04:48.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 8, with an accuracy of 100\%. Though should be noted that the size of this validation set consists of only 500 samples. Testing the whole testing set (15544 samples) achieves an accuracy of 0.9938.
%
%\subsection*{Attempt2}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11539 samples in total. Training time: 00:23:11.
%Training accuracy achieved 100\% before batch 50x5. Validation accuracy peaked at epoch 21, which achieved an accuracy of 0.984055. Applying this best validation model to testing data achieved an accuracy of 0.9833622. Lower accuracy could be the result of fewer data points for training - caused by having only a limited number of positives in the original dataset compared to the abundant negatives.
%
%\subsection*{Attempt3}
%
%Use original 7\% ratio. 41819 samples, with training/validation/testing in ratio 50:25:25. Have to be careful not to introduce too many negatives as this could impact what the model attempts to predict. E.g. if there are only 1\% positives, then an accuracy of 99\% could be achieved just by labelling all the samples as negative. Training time: 01:58:23.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 18, with an accuracy of 0.9979159, with the set containing 12955 samples. The testing set achieved an accuracy of 0.9972.
%
%\subsection*{Fixed Samples}
%
%Samples were changed. Previously, negative samples were chosen such that they were not lacunes, and also did not have a central pixel value of 0. However, some regions within the brain matter do have a central value of 0, and these can be some of the hardest points to distinguish.
%
%Negative sampling was changed to random points that were not lacunes, and the entire 9x9 central square is not 0 (to remove samples from outside the brain matter). 
%
%\subsection*{Attempt4}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11538 samples in total. Training time: 00:22:51.
%Training accuracy achieved 100\% at batch 28x5. Consistent 100\% by batch 50x5. Validation accuracy peaked at epochs 31 and 34, achieving an accuracy of 0.9899445. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9885615.
%
%Testing set was only 2885 samples. The number of true/false positives and negatives were:
%TP: 972
%TN: 1879
%FP: 30
%FN: 4
%
%Sensitivity is 972/(972+4) = 99.6\% and specificity is 1879/(1879+30) = 98.4\%.
%
%
%
%\subsection*{Attempt5}
%
%training/validation/testing with 8.77\% positives. 43854 samples in total. Training time: 01:13:17. Training accuracy achieved 100\% at batch 41x5. Consistently 100\% at batch 150x5. Validation accuracy peaked at epoch 22, which achieved an accuracy of 0.9983581. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9968989.
%Testing set comprised of 10964 samples. Of these, the number of true/false positives and negatives were calculated:
%
%TP: 955
%TN: 9969
%FP: 33
%FN: 7
%
%Sensitivity is 955/(955+7) = 99.3\%, and specificity is 9969/(9969+33) = 99.7\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{bibliography}