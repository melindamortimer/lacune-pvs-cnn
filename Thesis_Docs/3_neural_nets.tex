%\include{packages}
%\begin{document}

\chapter{Neural networks}\label{neuralNets-intro}

This chapter outlines the structure and workings of basic neural network models. Those who have a basic understanding of neural networks and wish to avoid the details surrounding model structure and minimisation algorithms can proceed to Chapter \ref{convnets} for an overview of convolutional neural networks and Chapter \ref{litrev} for existing lacune detection attempts.

% - Basic neural networks theory + perceptron
% - Loss functions
% - Minimisation functions
% 
% - Convolutional neural networks overview
% - Convolution layer
% - ReLU
% - Pooling
% - Fully connected layers
% 
% - 3D CNNs overview

Neural networks have become increasingly popular with advances in computing power and the availability of large data sets \citep{Goodfellow-et-al-2016}. They have been proven successful with \textsc{mri} discrimination tasks \citep{DouQ.2016ADoC, Yokoyama2007} and image classification tasks \citep{HeKaiming2015DDiR, AlexNet2012, GoogLeNet2015}. In some instances, neural networks have exhibited a higher image recognition accuracy than humans \citep{HeKaiming2015DDiR}.

The construction of neural networks has to be conducted with care. The resulting models are difficult to interpret and prone to overfitting. We now discuss the underlying structure of neural networks, cost minimisation, and techniques to avoid overfitting the data.

\section{Basic structure}\label{nnets-structure}

The structure of neural networks are analogous to that of neurons in the brain. Each brain cell receives a signal, conducts a small amount of processing, and passes the resulting signal to the next cell. Decisions made by the brain are the result of many neurons processing information in sequence and in parallel. Neural networks adopt a similar structure. Individual nodes receive values, apply a transformation, and pass the output to the next node. Several nodes may apply their transformations simultaneously. For this reason, the nodes are referred to as \textit{neurons}.

The structure of a neural network neuron is shown in Figure \ref{nnet-neuronfig}, and its goal is to quantify a particular feature of the input variables. Let ${\mathbf{x} = (x_1, x_2, \ldots, x_n)^\intercal}$ be a vector of $n$ input variables. Let $\mathbf{w} = (w_1, \ldots, w_n)^\intercal$ be a vector of weights. Let $b$ be an additional \textit{bias} variable, not to be confused with statistical bias. The bias term is a constant used to leverage the value of the neuron independently of the input variables. Let $\sigma(\cdot)$ be some function, known as an \textit{activation function}, which is generally nonlinear (see Section \ref{nnets-act}). Let $\cdot$ be the usual dot product. The output of a single neuron, known as an \textit{activation value}, is given by
\begin{align*}
	a = \sigma(\mathbf{w}\cdot\mathbf{x} + b).
\end{align*}


% Neuron structure diagram
\begin{figure}[hb]
\centering
\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,300); %set diagram left start at 0, and has height of 300

%Shape: Circle [id:dp7806506925261892] 
\draw   (270,154) .. controls (270,125.28) and (293.28,102) .. (322,102) .. controls (350.72,102) and (374,125.28) .. (374,154) .. controls (374,182.72) and (350.72,206) .. (322,206) .. controls (293.28,206) and (270,182.72) .. (270,154) -- cycle ;
%Straight Lines [id:da48045113953293783] 
\draw    (191,76) -- (270,154) ;


%Straight Lines [id:da20730706323067383] 
\draw    (190.92,117.33) -- (270,154) ;


%Straight Lines [id:da19390293965519045] 
\draw    (191.59,224.67) -- (270,154) ;


%Straight Lines [id:da5499170807272789] 
\draw    (374,154) -- (466.51,154) ;
\draw [shift={(468.51,154)}, rotate = 180] [color={rgb, 255:red, 0; green, 0; blue, 0 }  ][line width=0.75]    (10.93,-3.29) .. controls (6.95,-1.4) and (3.31,-0.3) .. (0,0) .. controls (3.31,0.3) and (6.95,1.4) .. (10.93,3.29)   ;

%Straight Lines [id:da7426731370805287] 
\draw    (270,154) -- (190.92,152.67) ;


%Straight Lines [id:da35133187079282135] 
\draw    (254,80.33) -- (270,154) ;

% Text Node
\draw (324,156) node   {$\mathbf{w} \cdot \mathbf{x} +b$};
% Text Node
\draw (180,72.33) node   {$x_{1}$};
% Text Node
\draw (180,113.33) node   {$x_{2}$};
% Text Node
\draw (180,185) node   {$\vdots $};
% Text Node
\draw (180,225.33) node   {$x_{n}$};
% Text Node
\draw (180,150.33) node   {$x_{3}$};
% Text Node
\draw (219,87) node   {$w_{1}$};
% Text Node
\draw (219,120) node   {$w_{2}$};
% Text Node
\draw (219,145) node   {$w_{3}$};
% Text Node
\draw (219,185) node   {$w_{n}$};
% Text Node
\draw (219,163) node   {$\vdots $};
% Text Node
\draw (254.67,70.33) node   {$b$};
% Text Node
\draw (408,145) node   {$\sigma ( \cdot )$};
% Text Node
\draw (480,155) node   {$a$};


\end{tikzpicture}
\caption{Neuron structure.}
\label{nnet-neuronfig}
\end{figure}

%% Diagram of single neuron
%\begin{figure}[ht]
%	\centering
%	\includegraphics[scale=0.5]{Images/3_neuron.png}
%	\caption{Neuron structure.}
%	\label{nnet-neuronfig}
%\end{figure}

\begin{example}
\label{nnets-and-eg}
A neuron can be used to express a logical \textsc{and} gate of two inputs. Let \textsc{true} be encoded to 1 and \textsc{false} be encoded to 0 so that $x_i \in \{0,1\}$ for $i=1,2$. Let $\mathbf{w} = (1, 1)^\intercal$ and $b = -1.5$. Let the activation function be the step function $\sigma(z) = 1$ for $z > 0$ and $\sigma(z) = 0$ otherwise. The resulting neuron has output
\begin{align*}
	a(\mathbf{x}) = \begin{cases}
		1 & ,x_1 + x_2 - 1.5 > 0\\
		0 & ,otherwise
	\end{cases},
\end{align*}
which will output 1 only when $\mathbf{x} = (1, 1)^\intercal$, satisfying the definition of an \textsc{and} gate.
\end{example}


A large number of these neurons can be arranged in layers to form a \textit{fully connected neural network}, as shown in Figure \ref{nnet-structurefig}. The inputs $\mathbf{x} = (x_1, x_2, \ldots, x_n)$ are used to predict the final response $y$. Let $\mathbf{a}^\ell$ be the vector of activation values of the $\ell$-th layer, where $\mathbf{a}^0 := \mathbf{x}$. Let $\mathbf{w}^\ell$ be the matrix of weights used to calculate $\mathbf{a}^\ell$, such that each row entry $\mathbf{w}_j^{\ell\intercal}$ is used to compute the $j$-th neuron of the $\ell$-th layer. Let $\mathbf{b}^\ell$ be the vector of biases of the $\ell$-th layer. Hence, the vector of activation values of the $\ell$-th layer can be expressed as
\begin{align}\label{nnets-act-eqn}
	\mathbf{a}^\ell = \sigma(\mathbf{w}^\ell \mathbf{a}^{\ell - 1} + \mathbf{b}^\ell),
\end{align}
where $\sigma(\cdot)$ applies the activation function to each element of the vector. Parameterising Equation \eqref{nnets-act-eqn} as $g_\ell(\mathbf{a}^{\ell - 1}; \mathbf{w}^\ell, \mathbf{b}^\ell)$, the final output of the neural network can be written as sequence of function compositions
\begin{align*}
	\mathbf{a}^L = g_L \circ g_{L-1} \circ \ldots \circ g_1.
\end{align*}

% Diagram of basic neural network structure. Input, 1 hidden layer and series of output layers
%\begin{figure}[ht]
%	\centering
%	\includegraphics[width=\textwidth]{Images/3_nnet_structure.jpg}
%	\caption{Basic neural network structure.}
%	\small Image taken from \url{`https://www.digitaltrends.com/cool-tech/what-is-an-artificial-neural-network/'}
%	\label{nnet-structurefig}
%\end{figure}
\begin{figure}[hb]
\centering
\begin{tikzpicture}[shorten >=1pt,->,draw=black!50, node distance=\layersep]
    \tikzstyle{every pin edge}=[<-,shorten <=1pt]
    \tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
    \tikzstyle{input neuron}=[neuron, fill=red!50];
    \tikzstyle{output neuron}=[neuron, fill=green!50];
    \tikzstyle{hidden neuron}=[neuron, fill=blue!50];
    \tikzstyle{annot} = [text width=4em, text centered]
	\def\layersep{2.5cm}
	
    % Draw the input layer nodes
    \foreach \name / \y in {1,...,3}
    % This is the same as writing \foreach \name / \y in {1/1,2/2,3/3,4/4}
        \node[input neuron, pin=left:$x_\y$] (I-\name) at (0,-\y) {};

    % Draw the hidden layer1 nodes
    \foreach \name / \y in {1,...,4}
        \path[yshift=0.5cm]
            node[hidden neuron] (Ha-\name) at (\layersep,-\y cm) {};
            
    % Draw the hidden layer2 nodes
    \foreach \name / \y in {1,...,4}
        \path[yshift=0.5cm]
            node[hidden neuron] (Hb-\name) at (2*\layersep,-\y cm) {};

    % Draw the output layer node
    \node[output neuron,pin={[pin edge={->}]right:$\hat{y}$}] (O) at (3*\layersep, -2) {};

    % Connect every node in the input layer with every node in the
    % hidden layer.
    \foreach \source in {1,...,3}
        \foreach \dest in {1,...,4}
            \path (I-\source) edge (Ha-\dest);
            
    % Connect every node in the hidden layer 1 with every node in
    % hidden layer 2.
    \foreach \source in {1,...,4}
        \foreach \dest in {1,...,4}
            \path (Ha-\source) edge (Hb-\dest);

    % Connect every node in the hidden layer with the output layer
    \foreach \source in {1,...,4}
        \path (Hb-\source) edge (O);

    % Annotate the layers
    \node[annot,above of=Ha-1, node distance=1cm] (hla) {Hidden layer 1};
    \node[annot,above of=Hb-1, node distance=1cm] (hlb) {Hidden layer 2};
    \node[annot,left of=hla] {Input layer};
    \node[annot,right of=hlb] {Output layer};
    
%    \node[annot,below of=Ha-4, node distance=1cm] (fa) {$f_1(\mathbf{v}_1)$};
%    \node[annot,below of=Hb-4, node distance=1cm] (fb) {$f_2(\mathbf{v}_2)$};
%    \node[annot,right of=fb, node distance=\layersep] (fo) {$f(\mathbf{v})$};

\end{tikzpicture}
\caption{Basic neural network structure. Each hidden neuron (blue) and the output neuron (green) has the structure shown in Figure \ref{nnet-neuronfig}. For simplicity, the weights and biases are not shown.}
\label{nnet-structurefig}
\end{figure}

During model fitting, the quality of the estimated weights $W$ and biases $B$ is quantified using some \textit{cost function} $C(W,B)$. A cost of 0 describes a perfect model fit. The weights and biases are trained to minimise this cost function with respect to $(W,B)$. However, since the number of variables can be very large, it is not always feasible to analytically minimise $C(W,B)$. Instead, we can approximate this minimisation via the Gradient-Descent algorithm (see Section \ref{nnets-graddesc}). This algorithm is repeated until the cost function falls within some tolerance $\tau$, or reaches the maximum number of iterations $n_T$.

%Let $\mathbf{w}^\ell_j$ to be the vector of weights used to compute the $j$-th neuron of the $\ell$-th layer, where $w_{ji}^\ell$ is the weight attributed to the $i$-th neuron of the $(\ell-1)$-th layer.

%Let $b_j^\ell$ be the bias of the $j$-th neuron in the $\ell$-th layer.

%Let $\mathbf{a}^\ell$ be the vector of activation values (outputs) for the $\ell$-th layer, where $a_j^\ell$ is the activation value of the $j$-th neuron of that layer. Set $z_j^\ell := \mathbf{w}_j^\ell\cdot \mathbf{a}^{\ell-1} + b_j^\ell= \sum_k w_{jk}^\ell a_k^{\ell-1} + b_j^\ell$ to be the values calculated before activation, and $\mathbf{z}^\ell$ to be the vector of $z_j^\ell$ for the $\ell$-th layer. 
%Set $Z$ to be the set of all $z_j^\ell$ in the whole network. 
%Let $L$ be the number of layers in the network so that $\mathbf{a}^L$ is the vector of activation values of the final output layer. 
%Let $X = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n\}$ be the set of $n$ training inputs. 
%Let the responses be $Y = \{\mathbf{y}_1, \mathbf{y}_2, \ldots, \mathbf{y}_n\}$, with $y_{ij}$ the $j$-th variable of $\mathbf{y}_i$. 
%Let $W$ and $B$ be the sets of all weights and biases as before. Let $\eta$ be the learning rate of the network. 
\section{Activation functions}\label{nnets-act}

Activation functions, denoted by $\sigma(\cdot)$, are applied just before neuron output. Without these functions, the network can be reduced to a linear combination of inputs. Activation functions avoid this by introducing nonlinearity into the model.

The simplest neuron type is the \textit{perceptron}, used in Example \ref{nnets-and-eg}. It is characterised by,
\begin{align*}
	x_i \in \{0,1\} \text{ and } \sigma(z) = \begin{cases}
		1, & z > 0 \\
		0, & z \le 0
	\end{cases}.
\end{align*}

To allow for continuous outputs, other common activation functions include the sigmoids, which are monotonically increasing, smooth approximations to the step function. Examples of these include the sigmoid function
\begin{align*}
	\sigma(z) = \dfrac{1}{1+e^{-z}},
\end{align*}
and hyperbolic tan function
\begin{align*}
	\sigma(z) = \tanh(z).
\end{align*}
Both the sigmoid and hyperbolic tan functions are common activation functions \citep{Goodfellow-et-al-2016}, however they are computationally expensive and so are not preferred for image recognition tasks, which contain a large number of variables \citep{LeCun2012, Nielson2015}.

The Rectified Linear Unit (ReLU) activation \citep{Maas2013} is given by,
\begin{align*}
	\sigma(z) = \max(0, z).
\end{align*}
The ReLU activation function is suitable for neural network optimisation as the function and its derivative are simplistic \citep{Goodfellow-et-al-2016}. The neuron is said to be \textit{active} for $z > 0$. In this region, the gradient is constant and large for positive input values, promoting training of the neuron's weights (see Section \ref{nnets-graddesc} on Gradient Descent). For $z \leq 0$, the neuron has a gradient of 0 and is deactivated, avoiding unnecessary parameter adjustments and computation. 

The softmax activation function,
\begin{align*}
	\sigma(z_j) = \dfrac{e^{z_j}}{\sum_ke^{z_k}},
\end{align*}
is used to structure a layer as a discrete probability distribution over $k$ classes. Hence, it is frequently used in the output layer of classification tasks.


\section{Cost functions}\label{nnets-cost}

Weights and biases are chosen such that they approximately minimise some cost function. \cite{Nielson2015} describes two restrictions on cost functions for them to be minimised using the Gradient Descent algorithm (see Section \ref{nnets-graddesc}). The first requirement is that the cost $C_X(\cdot)$ accrued from all samples $X$ equals the mean of costs accrued from $n$ distinct subsamples of $X$, denoted by $X_i$,
\begin{align*}
	C_X(W, B) = \dfrac{1}{n}\sum_{i=1}^n C_{X_i}(W,B).
\end{align*}

The second requirement is that the cost is independent of $\mathbf{a}^\ell$ for all $\ell < L$, noting that there remains a dependence on the final activation values. The most common cost function for classification tasks is \textit{cross-entropy} \citep{Nielson2015}, given by
\begin{align}\label{nnets-cross-entropy-eq}
	C(W,B) = -\dfrac{1}{n}\sum_{i=1}^{n}\sum_{j=1}^{n_L}\big[y_{ij}\log\big(a_j^L(\mathbf{x}_i,W,B)\big) + (1 - y_{ij})\log\big( (1 - a_j^L(\mathbf{x}_i,W,B))\big)\big],
\end{align}
where $n$ is the number of samples and $n_L$ is the number of neurons in the output layer.

We now show the efficacy of cross-entropy as a measure of misclassification cost by first determining the Maximum Likelihood Estimator (\textsc{mle}) of the weights $(W, B)$, and secondly by considering distributional differences between the data and the proposed model.

Let ${p}_{data}(\mathbf{x})$ be the unknown probability distribution of the data points \\${X = (\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_n)}$. Let $p_{model}(\mathbf{x}; \mathbf{\theta})$ be a family of probability distributions estimating the true $p_{data}(\mathbf{x})$. The \textsc{mle} of $\mathbf{\theta}$ is defined as
\begin{align*}
	\hat{\mathbf{\theta}}_{MLE} = \arg\max_{\mathbf{\theta}}\prod_{i=1}^np_{model}(\mathbf{x}_i;\mathbf{\theta}).
\end{align*}
This can be reformulated using the log-likelihood,
\begin{align*}
	\hat{\mathbf{\theta}}_{MLE} = \arg\max_{\mathbf{\theta}}\sum_{i=1}^n\log p_{model}(\mathbf{x}_i;\mathbf{\theta}).
\end{align*}
Dividing by $n$, the expression can be written in terms of expectation,
\begin{align}\label{nnets-mle-eq}
	\hat{\mathbf{\theta}}_{MLE} = \arg\max_{\mathbf{\theta}}\mathbb{E}_{\mathbf{x}\sim \hat{p}_{data}}\left[\log p_{model}(\mathbf{x}; \mathbf{\theta})\right].
\end{align}
This expression is equivalent to the minimisation of cross-entropy. The same minimisation occurs when considering the KL-divergence between $p_{model}$ and $p_{data}$.

\begin{definition}
	\textbf{Kullback-Leibler (KL)-divergence} is a measure of difference between two probability distributions $P(x)$ and $Q(x)$ given by
	\begin{align*}
		D_{KL}(P||Q) = \mathbb{E}_{x\sim P}\left[\log\dfrac{P(x)}{Q(x)}\right] = \mathbb{E}_{x\sim P}[\log P(x) - \log Q(x)],
	\end{align*}
	where $\cdot||\cdot$ denotes necessary ordering of the function inputs. Note that \textsc{kl}-divergence is not symmetric and is therefore not a metric.	
\end{definition}

Minimisation of the \textsc{kl}-divergence between $\hat{p}_{data}$ and $p_{model}$ is given by
\begin{align}\label{nnets-kl-eq}
	\hat{\theta}_{KL} = \arg\min_\theta\mathbb{E}_{\mathbf{x}\sim\hat{p}_{data}}\left[\log \hat{p}_{data}(\mathbf{x}) - \log p_{model}(\mathbf{x};\mathbf{\theta})\right].
\end{align}

Since $\hat{p}_{data}$ is not a function of $\mathbf{\theta}$, Equation \eqref{nnets-kl-eq} is equivalent to 
\begin{align}\label{nnets-kl-eq2}
	\hat\theta_{KL} = \arg\min_\theta-\mathbb{E}_{\mathbf{x}\sim\hat{p}_{data}}\left[\log p_{model}(\mathbf{x};\theta)\right].
\end{align}
Both the \textsc{mle} of $\theta$ (Equation \eqref{nnets-mle-eq}) and the minimiser of \textsc{kl}-divergence (Equation \eqref{nnets-kl-eq2}) are equivalent to the minimisation of cross-entropy (Equation \eqref{nnets-cross-entropy-eq}). This minimisation is generally conducted through the Gradient Descent algorithm.

% https://www.deeplearningbook.org/contents/ml.html P130
%The KL divergence between the empirical distribution $\hat{p}_{data}$ and 



%Unlike the quadratic cost, when the error is high, the learning rate is also high.

%Cost functions can include: the quadratic (simple), cross-entropy, regularisation (L1, L2, dropout, artificial expansion of training data).
%In using cross-entropy, the learning rate of the weight is controlled by the amount of error. This is unlike the quadratic cost function, which has a very slow learning rate when there is high error.
%Changing these can improve a model.
%Other improvements made by making better initialisations of weights, and better heuristics to choose hyper-parameters.

\section{The Gradient Descent algorithm}\label{nnets-graddesc}

Weights $W$ and biases $B$ are chosen such that they approximately minimise the cost function $C(W,B)$. Analytically minimising the cost function is possible, however the large number of variables makes this process slow, having time complexity $O(n^3)$ \citep{Marquardt1963}. The weights are instead estimated using the Gradient Descent algorithm, improving computation speed.

Let $\mathbf{v} = (W,B)$ of length $N$ be the vector of all weights and biases in the network. Gradient Descent considers the gradient of the cost with respect to the current weights and biases given by $\nabla C(\mathbf{v}) = \left(\dfrac{\partial C}{\partial v_1}, \dfrac{\partial C}{\partial v_2},\cdots, \dfrac{\partial C}{\partial v_N}\right)$. The algorithm shifts the weights and biases by a small amount $\Delta\mathbf{v}$ such that the cost decreases. Define $\mathbf{v}':= \mathbf{v} + \Delta\mathbf{v}$ to be the updated value of $\mathbf{v}$. Let $\Delta C(\mathbf{v})$ be the change in the cost function as a result of the shifted $\mathbf{v}'$. An approximation of $\Delta C(\mathbf{v})$ is given by the \textit{Total Differential Approximation}
\begin{align}
	\Delta C(\mathbf{v}) & \approx \dfrac{\partial C}{\partial v_1}\Delta v_1 + \dfrac{\partial C}{\partial v_2}\Delta v_2 + \cdots + \dfrac{\partial C}{\partial v_N}\Delta v_N\\
	& = \nabla C(\mathbf{v})\cdot \Delta \mathbf{v}.\label{nnets-total-diff-eq}
\end{align}
Note the distinction between the gradients denoted by $\nabla$ and small changes denoted by $\Delta$.

We update $\mathbf{v}'$ inductively such that the cost decreases by an amount proportional to a given \textit{learning rate} $\eta$, such that
\begin{align}\label{nnets-delta-v-eq}
	\Delta\mathbf{v} = -\eta \nabla C(\mathbf{v}), \quad \text{ where }\eta > 0.
\end{align}
Substituting Equation \eqref{nnets-delta-v-eq} into Equation \eqref{nnets-total-diff-eq}, we can write
\begin{align*}
	\Delta C(\mathbf{v}) \approx -\eta \|\nabla C(\mathbf{v})\|^2 \le 0.
\end{align*}

Thus if $\mathbf{v}' := \mathbf{v} - \eta \nabla C(\mathbf{v})$, the cost function will decrease. In training a neural network, the size of the shift can be set to $\|\Delta\mathbf{v}\| = \varepsilon$, for some $\varepsilon > 0$. It can be shown that the $\Delta\mathbf{v}$ which gives the greatest decrease in $C(\mathbf{v})$ is a function of $\varepsilon$ and $\nabla C$ \citep{Nielson2015}.

\begin{lemma}\label{nnets-graddescminproof}
	Let $\varepsilon > 0$ and suppose the size of the shift is constrained such that $\|\Delta\mathbf{v}\| = \varepsilon$. Then $\nabla C \cdot \Delta\mathbf{v}$ is minimised by $\Delta\mathbf{v} = -\eta\nabla C$, where $\eta = \dfrac{\varepsilon}{\|\nabla C\|}$.
\end{lemma}

\begin{proof}
	By the Cauchy-Schwarz Inequality, we have
	\begin{align}
			|\nabla C\cdot\Delta\mathbf{v}| \le \|\nabla C\|\times\|\Delta\mathbf{v}\|.
	\end{align}
	The minimum is given by,
	\begin{align}\label{nnets-gdesc-eq1}
		\min(\nabla C\cdot\Delta\mathbf{v}) = -\|\nabla C\|\times\|\Delta\mathbf{v}\|.
	\end{align}
	Substituting $||\Delta\mathbf{v}|| = \epsilon$ into Equation 	\eqref{nnets-gdesc-eq1}, we have
	\begin{align}\label{nnets-gdesc-eq2}
		\min(\nabla C\cdot\Delta\mathbf{v}) = -\varepsilon\|\nabla C\|.
	\end{align}
	Multiplying the numerator and denominator of Equation \eqref{nnets-gdesc-eq2} by $||\nabla C||$ gives
	\begin{align}\label{nnets-gdesc-eq3}
		\min(\nabla C\cdot\Delta\mathbf{v}) & = -\dfrac{\varepsilon\|\nabla C\|^2}{\|\nabla C\|}\\[1em]
		& = -\dfrac{\varepsilon\nabla C\cdot\nabla C}{\|\nabla C\|} \label{nnets-gdesc-eq4}.
	\end{align}
	By equating the coefficients of $\nabla C$ between Equations \eqref{nnets-gdesc-eq1} and \eqref{nnets-gdesc-eq4}, and noting that $\eta = \epsilon / ||\nabla C||$,
	\begin{align*}
		\operatorname*{arg\,min}_{\Delta\mathbf{v}}(\nabla C\cdot\Delta\mathbf{v}) & = -\dfrac{\varepsilon\nabla C}{\|\nabla C\|} \\
		& = -\eta\nabla C.
	\end{align*}	
\end{proof}

The Gradient Descent algorithm moves the coefficients in the direction of the steepest negative gradient. It assumes that the starting values are close enough to the global minimum to converge. If this assumption is not satisfied, the algorithm will instead converge to the local minimum. In this thesis, we are only concerned about estimating the local minimum as attaining the global minimum results in overfitting of the training data.

The Gradient Descent algorithm relies on the gradients of a large number of variables. We now discuss a number of techniques that improve algorithm efficiency, including the Stochastic Gradient Descent algorithm, the choice of learning rate and the Adam Optimiser.

%Since the updated weights and biases rely on the gradient, it should be noted that these gradients are required to be significantly different from zero. When this is not the case, the weights and biases stagnate and we have what is known as the Vanishing gradient problem, which is discussed further in Section \ref{nnet-vanishinggradprob}.

\subsection*{Stochastic Gradient Descent}\label{nnets-stochgraddesc}

Gradient Descent is a highly computationally intensive algorithm as the neural network has a very large number of weights. To improve training time, Stochastic Gradient Descent is a popular alternative as it improves the time complexity from $O(n^3)$ \citep{Marquardt1963} to $O(n)$ \citep{Robbins1951}. This algorithm improves learning speed by randomly selecting a small subset of the training set to learn from, referred to as a \textit{batch}. When training has been completed for that batch, a new batch of training inputs is randomly selected and the process repeats. Once training has occurred over all batches, it is said that an \textit{epoch} of training has been completed. In this manner, a relatively small number of samples is used for each weight adjustment. This drastically increases training speed whilst utilising all the information provided by the entire training set.

\subsection*{Learning rate}\label{nnets-learningrate}

During Gradient Descent, the change in gradients $\Delta\mathbf{v} = -\eta\nabla C$ moves the weights in the direction of the steepest negative gradient (see Section \ref{nnets-graddesc}). The learning rate $\eta$ controls the magnitude of this movement. If $\eta$ is too small, the adjustment of weights is also small and model training will take a long time. If $\eta$ is too large, the weights may be shifted too far and the values of $\mathbf{v}$ will continuously move past the local minimum, as shown in Figure \ref{nnets-high-learn-fig}.
% High learning rate during grad desc
\begin{figure}[ht]
\centering

\tikzset{every picture/.style={line width=0.75pt}} %set default line width to 0.75pt        

\begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
%uncomment if require: \path (0,291); %set diagram left start at 0, and has height of 291
\clip (85,15) rectangle (380,280);

%Shape: Parabola [id:dp09742535750627612] 
\draw  [color={rgb, 255:red, 74; green, 144; blue, 226 }  ,draw opacity=1 ][line width=2.25]  (90,18) .. controls (184.33,350) and (278.67,350) .. (373,18) ;

%Straight Lines [id:da887590123175323] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (107,71) -- (337,126) ;


%Straight Lines [id:da9052100651701044] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (138,158) -- (337,126) ;


%Straight Lines [id:da30101863302529275] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (138,158) -- (315,179) ;


%Straight Lines [id:da06604062534938782] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (161,207) -- (315,179) ;


%Straight Lines [id:da6697786125314551] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (161,207) -- (293,221) ;


%Straight Lines [id:da4053203519455636] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (178,232) -- (293,221) ;


%Straight Lines [id:da553912866083977] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (178,232) -- (278,242) ;


%Straight Lines [id:da1577171267791595] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (195,252) -- (278,242) ;


%Straight Lines [id:da21468778984608516] 
\draw [color={rgb, 255:red, 255; green, 0; blue, 0 }  ,draw opacity=1 ][line width=1.5]    (195,252) -- (267,253) ;

\end{tikzpicture}

\caption{High learning rates adjust weights past the local minimum}
\label{nnets-high-learn-fig}
\end{figure}

To aid in efficient training, it can be beneficial for learning rates to be adjusted during the training process. Higher initial learning rates allow for fast convergence toward the local minimum, and lower learning rates at later epochs give the algorithm greater precision.

\subsection*{Adam Optimiser}\label{nnets-adam}

The Adam Optimiser is an algorithm that introduces additional learning rates to the Gradient Descent algorithm. Different learning rates are applied to each of the weights in the neural network, allowing for faster convergence of the cost function to the minimum \citep{Kingma2014}.


\section{The Backpropagation algorithm}\label{nnets-backprop}

During Gradient Descent, it is necessary to calculate the gradient of the cost function with respect to the weights $(W,B)$. Let $z_j^\ell = \mathbf{w}_j^\ell \cdot \mathbf{a}^{\ell - 1} + b_j^\ell$ be the value of the $j$-th neuron of the $\ell$-th layer before the activation function is applied. The Backpropagation algorithm is an efficient algorithm that determines the $\dfrac{\partial C}{\partial z}$ and relates them to the rates of interest, $\dfrac{\partial C}{\partial W}$ and $\dfrac{\partial C}{\partial B}$ \citep{Nielson2015}. The following lemmas are necessary to understand the Backpropagation algorithm.

% http://neuralnetworksanddeeplearning.com/chap2.html

\begin{lemma}
	The error of the $j$-th neuron of the final output layer is
	\begin{align*}
		\delta_j^L = \dfrac{\partial C}{\partial a_j^L}\sigma'(z_j^L).
	\end{align*}
	The vector of output layer errors can be expressed in the matrix form,
	\begin{align}\label{nnets-bprop-eq1}
		\delta^L = \Sigma'(\mathbf{z}^L)\nabla_aC,
	\end{align}
where $\Sigma'(\cdot)$ is a matrix such that the $j$-th diagonal entry is $\sigma'(z_j^L)$ and all non-diagonal entries are 0.
\end{lemma}

\begin{proof}

Noting that $z_j^L$ is independent of $a_k^L$ for all $k \neq j$, we can write
	\begin{align*}
		\delta_j^L & = \dfrac{\partial C}{\partial z_j^L}\\
		& = \sum_k\dfrac{\partial C}{\partial a_k^L}\dfrac{\partial a_k^L}{\partial z_j^L} \\
		& = \dfrac{\partial C}{\partial a_j^L}\dfrac{\partial a_j^L}{\partial z_j^L}\\
		& = \dfrac{\partial C}{\partial a_j^L}\sigma'(z_j^L).
	\end{align*}
\end{proof}

\begin{lemma}
	The error $\delta^\ell_j$ of the $\ell$-th layer can be written in terms of the errors of the $(\ell + 1)$-th layer, 
	\begin{align*}
		\delta_j^\ell & = \sum_k\delta_k^{\ell+1}w_{kj}^{\ell+1}\sigma'(z_j^\ell).
	\end{align*}
	The vector of errors $\delta^\ell$ for the $\ell$-th layer can be expressed in the matrix form,
	\begin{align}\label{nnets-bprop-eq2}
		\delta^\ell & = \Sigma'(\mathbf{z}^\ell)(w^{\ell+1})^\intercal\delta^{\ell+1}
	\end{align}
		%& = \Sigma'(\mathbf{z}^\ell)(w^{\ell+1})^\intercal\cdots\Sigma'(\mathbf{z}^{L-1})(w^L)^\intercal\Sigma'(\mathbf{z}^L)\nabla_aC.
\end{lemma}

\begin{proof}
	Using the chain rule,
	\begin{align*}
		\delta_j^\ell & = \dfrac{\partial C}{\partial z_j^\ell}\\
		& = \sum_k\dfrac{\partial C}{\partial z_k^{\ell+1}}\dfrac{\partial z_k^{\ell+1}}{\partial z_j^\ell} \\
		& = \sum_k\delta_k^{\ell+1}\dfrac{\partial z_k^{\ell+1}}{\partial z_j^\ell}.
	\end{align*}
	Noting that $\dfrac{\partial z_k^{\ell+1}}{\partial z_j^\ell} = \dfrac{\partial}{\partial z_j^\ell}(\mathbf{w}_k^{\ell+1}\cdot \sigma(\mathbf{z}^\ell) + b_k^{\ell+1}) = w_{kj}^{\ell+1}\sigma'(z_j^\ell)$, we can write $\delta_j^\ell$ as
	\begin{align*}
		\delta_j^\ell & = \sum_k\delta_k^{\ell+1}w_{kj}^{\ell+1}\sigma'(z_j^\ell).
	\end{align*}
\end{proof}

\begin{lemma}
	The error $\delta_j^\ell$ is equivalent to the rate of change in the cost function with respect to the bias, such that
	\begin{align}\label{nnets-bprop-eq3}
		\delta_j^\ell = \dfrac{\partial C}{\partial b_j^\ell}.
	\end{align}
\end{lemma}
\begin{proof}
	Noting that $z_j^\ell$ is independent of $b_k^\ell$ for all $j \neq k$,
	\begin{align*}
		\delta_j^\ell & = \dfrac{\partial C}{\partial z_j^\ell} \\
		& = \sum_k\dfrac{\partial C}{\partial b_k^\ell}\dfrac{\partial b_k^\ell}{\partial z_j^\ell}\\
		& = \dfrac{\partial C}{\partial b_j^\ell}\dfrac{\partial b_j^\ell}{\partial z_j^\ell} \\
		& = \dfrac{\partial C}{\partial b_j^\ell}.
	\end{align*}
\end{proof}

\begin{lemma}
	The rate of change in cost with respect to any single weight value is given by
	\begin{align}\label{nnets-bprop-eq4}
		\dfrac{\partial C}{\partial w_{jk}^\ell} = a_k^{\ell-1}\delta_j^\ell.
	\end{align}
\end{lemma}
\begin{proof}
	By definition,
	\begin{align*}
		z_k^\ell & = \mathbf{w}_k^\ell\cdot\mathbf{a}^{\ell-1} + b_k^\ell
	\end{align*}
	Differentiating with respect to some weight $w_{km}^\ell$,
	\begin{align*}
		\dfrac{\partial z_k^\ell}{\partial w_{km}^\ell} = a_m^{\ell-1}.
	\end{align*}
	Using the chain rule,
	\begin{align*}	
		\dfrac{\partial C}{\partial w_{jk}^\ell} & = \dfrac{\partial C}{\partial z_j^\ell}\dfrac{\partial z_j^\ell}{\partial w_{jk}^\ell} = \delta_j^\ell a_k^{\ell-1}.
	\end{align*}
\end{proof}


The Backpropagation algorithm feeds an input $\mathbf{x}$ fowards through the network to determine the error in the final layer $\delta^L$ (Equation \eqref{nnets-bprop-eq1}).  The algorithm moves backwards through the network from the final layer back to the first hidden layer to determine the remaining errors in the network $\delta^\ell$, $\ell < L$ (Equation \eqref{nnets-bprop-eq2}). From these errors, the cost gradients in terms of the weights and biases can be calculated (Equations \eqref{nnets-bprop-eq3} and \eqref{nnets-bprop-eq4}). The complete algorithm is detailed in Algorithm \ref{nnets-bprop-alg}.
%\begin{enumerate}
%	\item \textbf{Inputs}. Enter observations $x_1,\ldots,x_n$ to retrieve $\mathbf{a}^1$.
%	\item \textbf{Feedforward}. Compute $\mathbf{z}^\ell = \mathbf{w}^\ell\mathbf{a}^{\ell-1} + \mathbf{b}^\ell$ for each $l = 2, 3,\ldots,L$.
%	\item \textbf{Output error}. Compute $\delta^L = \Sigma'(\mathbf{z}^L)\nabla_aC$.
%	\item \textbf{Backpropagation}. For $\ell = L-1, \ldots, 2$, compute $\delta^\ell =  \Sigma'(\mathbf{z}^\ell)(w^{\ell+1})^\intercal\delta^{\ell+1}$.
%	\item \textbf{Gradients}. Compute $\dfrac{\partial C}{\partial b_j^\ell} = \delta_j^\ell$ and $\dfrac{\partial C}{\partial w_{jk}^\ell} = a_k^{\ell-1}\delta_j^\ell$.
%\end{enumerate}

\begin{algorithm}[H]
\label{nnets-bprop-alg}
\SetAlgoLined
\For{$\mathbf{x}$ in $\mathbf{x}_1, \mathbf{x}_2,\ldots,\mathbf{x}_n$}{
	$\mathbf{a}^1 = \mathbf{w}^1\cdot\mathbf{x}+\mathbf{b}^1$\\
	\underline{\textsc{Feedforward}}\\
	\For{$\ell$ in $2,3,\ldots,L$} {
		$\mathbf{z}^\ell = \mathbf{w}^\ell\cdot\mathbf{a}^{\ell-1} + \mathbf{b}^\ell$
	}
	$\delta^L = \Sigma'(\mathbf{z}^L)\nabla_aC$\\
	\underline{\textsc{Backpropagation}}\\
	\For{$\ell$ in $L-1, L-2, \ldots, 2$} {
		$\delta^\ell =  \Sigma'(\mathbf{z}^\ell)(w^{\ell+1})^\intercal\delta^{\ell+1}$
	}
	$\dfrac{\partial C}{\partial b_j^\ell} = \delta_j^\ell$\\[1em]
	$\dfrac{\partial C}{\partial w_{jk}^\ell} = a_k^{\ell-1}\delta_j^\ell$
}
\caption{Calculation of gradients during backpropagation}
\end{algorithm}




\subsection*{Vanishing gradient problem}\label{nnet-vanishinggradprob}
The \textit{vanishing gradient problem} refers to the stagnation of weights and biases during the Gradient Descent algorithm caused by very small activation function gradients. The errors $\delta^\ell$ calculated during backpropagation are dependent on the gradient of the activation function (see Equation \eqref{nnets-bprop-eq1}). If this gradient becomes too small, then the errors $\delta$ and movement of weights $\Delta\mathbf{v}$ also become small (see Equation \eqref{nnets-delta-v-eq}), limiting the movement of weights and biases. For the sigmoid activation function,
\begin{align*}
	\sigma'(z) = e^{-z}(1+e^{-z})^{-2},
\end{align*}
and for the hyperbolic tan activation function,
\begin{align*}
	\sigma'(z) = 1-\tanh^2(z).
\end{align*}
For both the sigmoid and tanh activation functions, the limit as $z\rightarrow\pm\infty$ yields 
\begin{align*}
	\lim_{z\rightarrow\pm\infty}\sigma'(z)= 0,
\end{align*}
causing stagnation of the movement of weights and biases. The ReLU activation function avoids this by having gradient
\begin{align*}
	\sigma'(z) = \begin{cases}
		1, & z > 0 \\
		0, & z < 0 \\
		\text{undefined}, & z = 0
	\end{cases}.
\end{align*}
% Paper: https://www.utc.fr/~bordesan/dokuwiki/_media/en/glorot10nipsworkshop.pdf
Neurons that return a positive $z_j^\ell$ have a large gradient and so remain actively trained. If the value of $z_j^\ell$ becomes negative, the activation will have a gradient of 0 and the neuron will cease training. It should be noted that, in practice, the gradient of ReLU activation will output 0 at $z = 0$ for simplicity \citep{Goodfellow-et-al-2016}. 


%If this occurs in the whole network, the ReLU activation can be replaced with the \textit{leaky ReLU},
%\[
%	f(z) = \max(x, ax), \quad a \le 1.
%\]
%This activation has gradient function
%\[
%	f '(z) = \begin{cases}
%		1 & z > 0 \\
%		a & z < 0
%	\end{cases},
%\]
%which avoids the zero gradient for negative $z$.

\section{Weight initialisation}

The initialisation of weights and biases affects the rate and quality of training \citep{Mishkin2016}. Intuitively, if the weights are initialised close to the final output values, it will be faster to train. Conversely, weights that are initialised poorly may take a longer time to train to the same accuracy, or the local minimum of the cost function may not converge close to the global minimum and produce a poor model.

If the weights in two neurons are similar, using the same activation function will result in similar outputs. Therefore, initialising weights and biases with the same values introduces a lot of redundant calculations. It is commonplace for weights and biases to be initialised randomly \citep{Nielson2015}. An example of an initialisation distribution is the truncated normal distribution. Let $X\sim\mathcal{N}(\mu,\sigma^2)$ and $-\infty \le a < b \le \infty$. For $x \in (a,b)$, the truncated normal $X$ has probability density function (\textsc{pdf})
\begin{align*}
	f_X(x;\mu, \sigma^2,a,b) = \dfrac{\phi\big(\frac{x-\mu}{\sigma}\big)}{f\bigg(\Phi\big(\frac{b-\mu}{\sigma}\big) - \Phi\big(\frac{a-\mu}{\sigma}\big)\bigg)},
\end{align*}
where $\phi$ and $\Phi$ are the standard Normal \textsc{pdf} and cumulative density function (\textsc{cdf}) respectively. This method initialises the weights with a small amount of noise, with truncation of the Normal distribution helping to avoid vanishing gradients should the sigmoid or hyperbolic tan activation functions be used (see Section \ref{nnet-vanishinggradprob}).

In the case that the ReLU activation function is used, the He Method \citep{HeKaiming2015DDiR} is a common initialisation algorithm. This method recognises the use of the ReLU activation function and scales its initialisations dependent on the size of the previous layer. According to \cite{HeKaiming2015DDiR}, the method helps to constrain the variance of the weights during training and thus improves training time and quality. The He Method samples from the Normal distribution $\mathcal{N}\left(0, 2/d_{\ell-1}\right)$, where $d_{\ell-1}$ is the number of neurons in the $(\ell-1)$-th layer.

%\begin{align}
%	w = Z\times\sqrt{\dfrac{2}{d_{\ell-1}}},\quad Z\sim\mathcal{N}(0,1),
%\end{align}

%Initial weights and biases chosen using independent Gaussian random variables, with mean 0, sd 1. But this is quite a broad distribution. Can become relatively likely for neurons to become saturated (corrections are minuscule). Instead, try a standard deviation of $1/\sqrt(n)$.

%Choosing hyper-parameters:
%It can be difficult to determine what to change with so many parameters in play at once. This is particularly the case with large amounts of data or complex models. Change the learning rate? Number of hidden neurons? Number of layers? 
%General strategy is to start simple. First just try to get ANY non-trivial result. E.g. for MNIST, just isolate 0/1 images. Start without hidden layers, just to test it out. Once there are non-trivial results, start building these more complex structures.

\section{Regularisation}\label{nnet-reg}

Due to the large number of parameters to be estimated, parameters generally outnumber observations. During training, observations must be reused and so neural network models have a tendency to overfit the data \citep{Nielson2015}. \textit{Regularisation} is the process of mitigating the tendency to overfit data by applying penalties to the network as it trains. We now cover some regularisation techniques, including L2-regularisation, dropout, batch normalisation, data expansion and early stopping.

\subsection*{L2-regularisation}\label{nnet-l2reg}

% Regularisation & Dropout

Penalties can be applied to the cost function to artificially increase the error and promote model training. \textit{L2-regularisation} adds a penalty term of scaled model weights, similar to that of Ridge regression. The scaling of the weights is controlled by the regularisation parameter $\lambda$ to give the updated cost function
\begin{align*}
	C(W,B) + \dfrac{\lambda}{2n}\sum_{v\in (W,B)}v^2.
\end{align*}

%Another common regularisation is L1, or Lasso Regression, which takes the sum of the absolute weights,
%\begin{align}
%	C(W,B) + \dfrac{\lambda}{n}\sum_{w\in W}|w|.
%\end{align}

\subsection*{Dropout}\label{nnet-dropout}
During training, it is possible for individual neurons to become sensitive to patterns present in specific observations. Dropout layers \citep{Srivastava2014} assign each connected neuron connected with a preset probability $p$ of being deactivated, regardless of their input. This ensures that relevant data features are spread through several neurons, and that the impact of an individual neuron does not strongly affect the final result.

\subsection*{Batch normalisation}

Deep neural network models have a large number of variables and so require a longer training time. Training through many epochs can cause some activation values to become very large. Batch normalisation involves normalising the activation values within each training batch. This stops the activations from becoming too large and also avoids overfitting by adding some noise to the weights and ensuring that critical features are spread across nodes \citep{IoffeSergey2015}.

\subsection*{Data expansion}

Neural networks require a very large sample size to avoid overfitting as there are a large number of variables. If the number of samples is not large enough, additional data samples can be created by augmenting the existing data set \citep{PerezLuis2017}. Common augmentations for image data include flipping horizontally and vertically, rotation and cropping. Note that not all augmentations will be valid for each context. For instance, images of text should not be flipped. 

\subsection*{Early stopping}\label{nnets-earlystop}

Lengthy model training can cause extensive overfitting of the data. Early stopping of finalises the model before severe overfitting takes place and testing performance drops \citep{Prechelt2012}. A common early stopping method is to save snapshots of the model during the training process and select the model that performs the best on the validation set.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\include{bibliography}