\include{packages}
\begin{document}

\chapter{Results}\label{results}

\section{Final model structure}

The final simplified model has a similar structure to that of the candidate detection model by \cite{GhafoorianM.2017Dml3}. The overall structure is shown in Figure \ref{results-model-fig}. The input data contains two axial images: T1-weighted and \textsc{flair} images. Each of the images has resolution 51$\times$51 pixels and is centred at the same coordinate. The model classifies the central pixel as either a positive (lacune) or negative (non-lacune) by returning the classification with the highest probability.
% Simplified model structure
\begin{figure}[ht]
	\centering
	\includegraphics[width=\linewidth]{Images/7_simplified_model.png}
	\caption{Simplified model structure.}
	\small Image adapted from \cite{GhafoorianM.2017Dml3}
	\label{results-model-fig}
\end{figure}

The model consists of four convolutional layers of size 20, 40, 80 and 110, with filter sizes 7$\times$7, 5$\times$5, 3$\times$3 and 3$\times$3 respectively. A single max pooling layer is placed after the first convolutional layer, with size 2$\times$2 and stride 2. The convolutional layers are followed by three fully connected layers of size 300, 200 and 2. The final layer has softmax activation to output a probability distribution. All other layers have ReLU activation in order to avoid the vanishing gradient problem (see Section \ref{nnet-vanishinggradprob}). All neurons undergo batch normalisation and are initialised using the He method \citep{HeKaiming2015DDiR}. A dropout rate of 0.3 was applied to the fully connected layers.

Training was conducted using stochastic gradient descent with the Adam optimiser and a batch size of 128. A decaying learning rate was set from $5\times10^{-4}$ to $1\times10^{-6}$ by the 40th epoch. Cross-entropy cost was used with L2-regularisation of penalty rate $1\times10^{-4}$. An early stopping mechanism was implemented such that the model is tested against the validation set at the end of each epoch. If the validation accuracy improves, the model is saved. Once 40 epochs have run, the final saved model will have the highest validation accuracy.

Two models were compared to assess the impact of soft tissue extraction on the quality of lacune detection. Soft tissue extraction creates a mask that retains voxels with an over 50\% probability of containing brain tissue. This threshold is low and the mask has a tendency to remove voxels that contain a high probability of \textsc{csf} content. This includes voxels that contain lacunes and voxels close to the edge of the brain matter. Hence, the resulting images have a tendency to increase the perceived size of lacunes in the T1-weighted images, and additionally significant contextual loss occurs for lacunes in regions close to the edge of the brain matter. We conduct two models of differing image inputs to test to the severity of this information loss and how it affects lacune detection.

Two models were trained using different T1-weighted images. The first was developed using the soft tissue extracted T1-weighted scans. 


Two models were trained using different positive-negative response ratios. The first model was developed using the positive-negative response ratio described by \cite{GhafoorianM.2017Dml3}: one-third positive and two-thirds negative. There are only 3,846 positive samples in the MAS data set, allowing for a random selection of 7,692 negative samples, thus giving a total sample size of 11,538. These samples were split into three data sets: training, validation and testing. Splitting the data into these groups at a ratio of 50:25:25 yielded sample sizes 5,769, 2,884 and 2,885 respectively.

The second model used the entire developed set of positive and negative lacune samples. This consisted of 3,846 positive and 39,983 negative samples to give a total sample size of 43,829. Positive samples encompass 8.78\% of data samples. Similarly to the first data set, the data was split into training, validation and testing sets using a ratio of 50:25:25. This yielded sample sizes 21,914, 10,957 and 10,958 respectively. All data sets were saved as R data files (\texttt{.Rda}).



\section{Training environment}

Model code was developed in R (v3.5.0) using RStudio (v1.1.453). The neural network was built and trained using Tensorflow (v1.10.0) through the R interface \texttt{tensorflow} (v1.8). The model was trained on a Linux machine running Ubuntu (release 16.04). The Tensorflow model was trained on the machine's CPU, an Intel(R) Core(TM) i7-4790 CPU 3.60GHz, with 16GB of RAM. 

\section{Results}

\subsection*{First model}

The first model generated has the same positive-negative sample ratio as outlined by \cite{GhafoorianM.2017Dml3}. The data consists of one-third positives and two-thirds negatives. The model was trained for 40 epochs over 24 minutes, adjusting the network weights with each batch of 128 samples.

% Training accuracy
\begin{figure}[b]
	\centering
	\includegraphics[width=\textwidth]{Images/7_train_acc4.pdf}
	\caption{Training data accuracy logged every 5 batches.}
	\label{results-train-acc4-fig}
\end{figure}

Reusing the same data samples a large number of times introduces overfitting into the network \citep{Goodfellow-et-al-2016}. Overfitting can be observed when the accuracy of the training data set becomes very high, whereas the accuracy of the validation and testing data sets are low. Hence assessing model training accuracy can efficiently convey early model improvement; however it is not indicative of the model's performance when given new data.

Training accuracy was calculated every 5 batches, where each batch contained 128 samples. The resulting accuracies are shown in Figure \ref{results-train-acc4-fig}. There is a rapid increase in training accuracy within the first 100 batches. The model improvement occurs in steps as new features are found and more precise weight changes are made with the lowering learning rate \citep{Folly2009, Nielson2015}. The model reports a 100\% training accuracy consistently by batch 300. A majority of the model training occurs very early. In later training epochs, the training cost and learning rates are low and so the change in weights is also low (see Section \ref{nnets-backprop} on Backpropagation).

% Validation accuracy
\begin{figure}[b]
	\centering
	\includegraphics[width=\textwidth]{Images/7_valid_acc4.pdf}
	\caption{Validation data accuracy logged after each epoch.}
	\label{results-valid-acc4-fig}
\end{figure}

The model's validation set accuracy was tested after each epoch. This data set was not used for training and is used to assess the model's performance given new unseen data. The resulting validation accuracies are shown in Figure \ref{results-valid-acc4-fig}. Validation accuracy increases rapidly within the first 10 epochs. The maximum validation accuracy occurred at epochs 18, 22 and 25, achieving an accuracy of 99.2\%. The model was saved at epoch 18 to help minimise overfitting that may occur at later epochs. Applying this best validation accuracy model to the test data set resulted in an accuracy of 99.4\%.



Though overall accuracy is a strong indicator of model performance, some types of inaccuracy can be more detrimental than others. In the identification of lacunes, it is preferable to have a larger number of false candidate lacunes than misclassify a true lacune. As a result, models will be compared by considering actual and predicted classification differences. Using this scheme, there are four outcomes: true-positive, true-negative, false-positive, and false-negative. These outcomes can be formatted into a \textit{confusion matrix}, as shown in Table \ref{results-confmat4-tab}.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lll@{}}
	\toprule[1.5pt]
	& Positive & Negative\\
	\midrule
	True & 1004 & 1863\\
	False & 16 & 2\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Confusion matrix of the Model 1 testing data set.}
	\label{results-confmat4-tab}
\end{table}

The sensitivity of the model to lacunes is 99.6\% and specificity (correct classification of negative samples) of non-lacunes is 98.4\%.

\subsection*{Second model}

The second model was built using all of the generated positive and negative samples as described in Section \ref{data-samples}. The resulting training accuracy is shown in Figure \ref{results-train-acc5-fig}. The additional noise present in comparison to the previous model is the result of the larger sample size, rising from 5,800 to 22,000 samples. Note that the batch size and number of training epochs remained at 128 and 40 respectively, resulting in a larger number of batches processed during training. Model training duration was 85 minutes. The behaviour of this model was similar to that of the previous model, with a steep increase in training accuracy in early epochs followed by consistent correct classifications. Training accuracy was consistently at 100\% by batch 1000. 

% Training accuracy
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/7_train_acc5.pdf}
	\caption{Training data accuracy logged every 5 batches.}
	\label{results-train-acc5-fig}
\end{figure}

The performance of the model on the validation set is given in Figure \ref{results-valid-acc5-fig}. Validation accuracy was maximised at epochs 28 and 34, achieving an accuracy of 99.8\%. The model was saved at the earlier epoch 28 to reduce overfitting.

% Validation accuracy
\begin{figure}[ht]
	\centering
	\includegraphics[width=\textwidth]{Images/7_valid_acc5.pdf}
	\caption{Validation data accuracy logged after each epoch}
	\label{results-valid-acc5-fig}
\end{figure}

Applying this best validation accuracy model to the test set achieved an accuracy of 99.8\%. The confusion matrix is given by Table \ref{results-confmat5-tab}, to give a sensitivity of 99.9\% and specificity of 99.7\%.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lll@{}}
	\toprule[1.5pt]
	& Positive & Negative\\
	\midrule
	True & 961 & 9977\\
	False & 19 & 1\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Confusion matrix of the Model 2 testing data set.}
	\label{results-confmat5-tab}
\end{table}

\subsection*{Model comparisons}

We now discuss the effect of the positive-negative sample ratio and sample size on model sensitivities and specificities. An outline of the models is given in Table \ref{results-sens-spec-tab}. We observe that the second trained model has a higher sensitivity and specificity than that of the first model. We conduct a hypothesis test on the differences between the two models.

\begin{table}[ht]
	\centering
	\begin{tabular}{@{}lll@{}}
	\toprule[1.5pt]
	& Sensitivity & Specificity\\
	\midrule
	Model 1 & 0.9980 (1004/1006) & 0.9915 (1863/1879)\\
	Model 2 & 0.9990 (961/962) & 0.9981 (9977/9996)\\
	\bottomrule[1.5pt]\\
	\end{tabular}
	\caption{Sensitivities and specificities of both models.}
	\label{results-sens-spec-tab}
\end{table}

First, we test the difference in sensitivity proportions. The Central Limit Theorem (\textsc{clt}) is not applicable as the number of false-negatives is too small in both models. Instead, we use Fisher's Exact test. Let $X_1$ and $X_2$ be the number of lacunes correctly identified under the first and second models respectively. Let $n_1$ and $n_2$ be their respective sample sizes. Under the null hypothesis, for fixed $X_1 + X_2$, $X_1$ and $X_2$ have a hypergeometric distribution with probability
\begin{align*}
	P(X_1 = x_1, X_2 = x_2) = \dfrac{\dbinom{n_1}{x_1}\dbinom{n_2}{x_2}}{\dbinom{n_1 + n_2}{x_1 + x_2}}.
\end{align*}

The resulting p-value is 1, and we conclude that the two sensitivity proportions are not significantly different.

We now test the difference in specificity rates. We hypothesise that both sample proportions come from sampling distributions with the same probability. We can apply a $z$-test since we have sufficiently large number of observations and the data are independent. Let $X_1$ and $X_2$ be the number of correctly classified negative samples from the first and second models respectively. Let $n_1$ and $n_2$ be the total number of negative samples, so that $\hat{p}_1 = \dfrac{X_1}{n_1}$ and $\hat{p}_2 = \dfrac{X_2}{n_2}$ are the observed specificities. The observed pooled specificity proportion is
\begin{align*}
	\hat p_{pooled} = \dfrac{1863+9977}{1879+9996} = 0.9970526.
\end{align*}
From the \textsc{clt},
\begin{align*}
	Z = \dfrac{\hat{p}_1 - \hat{p}_2}{\sqrt{p_{pooled}(1 - p_{pooled})\left(\frac{1}{n_1} + \frac{1}{n_2}\right)}} \approx \mathcal{N}(0,1).
\end{align*}
The resulting test statistic is $z = -4.852599$. The probability of the observed proportion difference or lower is given by $P(Z < z) = 6.09\times10^{-7}$. This is well below the 5\% significance level and it is concluded that the specificity of the second model is higher than that of the first.

It has been shown that the second model did not have a significantly different sensitivity rate, but had an increased specificity. It is likely that this increase is due to the larger sample size rather than the effect of the positive-negative ratio. The model is exposed to a larger number of negative samples and so is able to correctly classify a greater number of them.

The resulting sensitivity and specificity rates of the final model are very high, therefore the simplicity of the final model can be maintained by removing the false-positive reduction model. \citep{GhafoorianM.2017Dml3} reduced false-positives by training three separate three-dimensional \textsc{cnn}s on candidate lacune samples. These outputs were concatenated with seven additional location-based variables into a fully-connected neural network. This process introduces a large number of extra weights, and training and usage time. Additionally, extra data processing is required to generate the three-dimensional samples and to retrieve the location-based variables.

Our model therefore removes the false-positive reduction component of the model by \cite{GhafoorianM.2017Dml3}, and achieves a final sensitivity rate of 99.9\% and specificity rate of 99.8\%.

We now discuss the features of samples classified correctly and incorrectly. Figure \ref{results-tp} demonstrates an example of a sample that was correctly classified as a lacune. True-positive samples generally consist of a low or zero intensity in the T1-weighted image, and a dark region surrounded by a hyperintense rim in the \textsc{flair} image. False-positive classifications, as shown in Figures \ref{results-fp1} and \ref{results-fp2}, occurred with samples that were near low or zero intensities in the T1-weighted image, and also appeared close to a dark structure with hyperintense rim in the \textsc{flair}. However, the hyperintense rims visible in these samples were not the result of lacunes, often occurring near the outer edge of the brain matter or adjacent to the cerebral ventricles. False-negative samples, shown in Figure \ref{results-fn}, are rare and generally occur at the edges of small lacunes that have little presence in the \textsc{flair} imaging.

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/7_TP_t1_flair.png}
\caption{True-positive sample. The sample has a zero intensity in the T1-weighted image. A distinct dark region with hyperintense rim is visible in the \textsc{flair} image.}
\label{results-tp}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/7_FP_t1_flair.png}
\caption{False-positive sample. The sample occurs near the edge of the brain matter and so has a low intensity in the T1-weighted scans. The false-positive classification may arise from the small amount of hyperintensity visible in the \textsc{flair} image.}
\label{results-fp1}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/7_FP_t1_flair2.png}
\caption{False-positive sample. The sample occurs next to the cerebral ventricles, which have a zero intensity in the T1-weighted image. A dark region with a hyperintense edge is visible in the \textsc{flair} image.}
\label{results-fp2}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=\linewidth]{Images/7_FN_t1_flair.png}
\caption{False-negative sample. The lacune has a diameter of only 3 mm, and is visible in the T1-weighted image. The \textsc{flair} image has no visible dark region around the lacune or hyperintense rim.}
\label{results-fn}
\end{figure}

%\section{Reference Model Results}
%
%Results from running Ghafoorian's model. Training time, and final sensitivity and average false-positives per slice.
%
%\subsection*{Attempt1}
%
%training/testing in ratio 70:30. Here, only 7\% of data consists of positives. 41819 samples in total. Validation occurs on first 500 of the testing set. Training time: 02:04:48.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 8, with an accuracy of 100\%. Though should be noted that the size of this validation set consists of only 500 samples. Testing the whole testing set (15544 samples) achieves an accuracy of 0.9938.
%
%\subsection*{Attempt2}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11539 samples in total. Training time: 00:23:11.
%Training accuracy achieved 100\% before batch 50x5. Validation accuracy peaked at epoch 21, which achieved an accuracy of 0.984055. Applying this best validation model to testing data achieved an accuracy of 0.9833622. Lower accuracy could be the result of fewer data points for training - caused by having only a limited number of positives in the original data set compared to the abundant negatives.
%
%\subsection*{Attempt3}
%
%Use original 7\% ratio. 41819 samples, with training/validation/testing in ratio 50:25:25. Have to be careful not to introduce too many negatives as this could impact what the model attempts to predict. E.g. if there are only 1\% positives, then an accuracy of 99\% could be achieved just by labelling all the samples as negative. Training time: 01:58:23.
%Training accuracy achieved 100\% before batch 100x5. Validation accuracy peaked at epoch 18, with an accuracy of 0.9979159, with the set containing 12955 samples. The testing set achieved an accuracy of 0.9972.
%
%\subsection*{Fixed Samples}
%
%Samples were changed. Previously, negative samples were chosen such that they were not lacunes, and also did not have a central pixel value of 0. However, some regions within the brain matter do have a central value of 0, and these can be some of the hardest points to distinguish.
%
%Negative sampling was changed to random points that were not lacunes, and the entire 9x9 central square is not 0 (to remove samples from outside the brain matter). 
%
%\subsection*{Attempt4}
%
%training/validation/testing in ratio 50:25:25. 1/3 of data was positives. 11538 samples in total. Training time: 00:22:51.
%Training accuracy achieved 100\% at batch 28x5. Consistent 100\% by batch 50x5. Validation accuracy peaked at epochs 31 and 34, achieving an accuracy of 0.9899445. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9885615.
%
%Testing set was only 2885 samples. The number of true/false-positives and negatives were:
%TP: 972
%TN: 1879
%FP: 30
%FN: 4
%
%Sensitivity is 972/(972+4) = 99.6\% and specificity is 1879/(1879+30) = 98.4\%.
%
%
%
%\subsection*{Attempt5}
%
%training/validation/testing with 8.77\% positives. 43854 samples in total. Training time: 01:13:17. Training accuracy achieved 100\% at batch 41x5=205. Consistently 100\% at batch 150x5=750. Validation accuracy peaked at epoch 22, which achieved an accuracy of 0.9983581. Applying this best validation accuracy model to the test set achieved an accuracy of 0.9968989.
%Testing set comprised of 10964 samples. Of these, the number of true/false-positives and negatives were calculated:
%
%TP: 955
%TN: 9969
%FP: 33
%FN: 7
%
%Sensitivity is 955/(955+7) = 99.3\%, and specificity is 9969/(9969+33) = 99.7\%.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\include{bibliography}
\end{document}